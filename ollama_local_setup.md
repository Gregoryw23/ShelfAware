# Local Ollama Setup for Semantic Search and RAG

This document outlines the steps to set up Ollama locally for both embedding generation and natural language summarization, replacing external API calls. This allows for a completely local, free, and privacy-preserving RAG pipeline.

## 1. Install Ollama

First, you need to install the Ollama server on your machine.

*   **Download and Install**: Go to the official Ollama website: [ollama.com/download](https://ollama.com/download)
*   Follow the instructions for your operating system (macOS, Linux, Windows).

Once installed, Ollama runs as a background service on your machine, typically listening on `http://localhost:11434`.

## 2. Pull Necessary Models

Next, you need to download the specific models that Ollama will serve. We'll need one for embeddings and one for natural language generation (a small LLM).

**Important**: Use the `ollama pull` command to download the model weights to your system. While `ollama run` can automatically pull a model if it's not present, `ollama pull` is the explicit command for downloading models as part of your setup.

Open your terminal and run the following commands:

*   **For Embeddings (e.g., `embeddinggemma`)**:
    ```bash
    ollama pull embeddinggemma
    ```
    `embeddinggemma` is a 300M parameter, state-of-the-art open embedding model from Google.

*   **For Natural Language Generation**: You can choose between `gemma3:1b` or `gemma3:270m` for summarization. When pulling, ensure you specify the full tag to get the correct model size. Pull your preferred model:
    ```bash
    ollama pull gemma3:1b
    # OR
    ollama pull gemma3:270m
    ```
    `gemma3:1b` and `gemma3:270m` are both small, capable LLMs suitable for summarization. `gemma3:1b` is slightly larger and potentially more capable than `gemma3:270m`.

## 3. Install Python Dependencies

Ensure you have the `ollama` Python client installed in your project's virtual environment.

```bash
pip install ollama
```

## 4. Check `app/services/chroma_service.py` is available

You need to check and update `app/services/chroma_service.py` to use the local Ollama instance for both embedding generation and natural language summarization.

## 5. Run the Application

Before running your FastAPI application, **ensure your local Ollama server is running in the background**. It usually starts automatically after installation, but you can verify its status or restart it if needed (refer to Ollama documentation for specific commands for your OS).

Then, start your FastAPI application as usual:

```bash
uvicorn app.main:app --reload
```

## 6. Verification

1.  **Access Swagger UI**: Open your browser to `http://localhost:8000/docs`.
2.  **Add Books to ChromaDB**:
    *   Use the `POST /chroma` endpoint.
    *   Provide book data (e.g., from `sample_book_data.md`).
    *   You should get a success message.
3.  **Search for Books**:
    *   Use the `GET /chroma/similarities` endpoint with a query (e.g., `query=Deep Learning`).
    *   Verify that relevant books are returned.
4.  **Get AI-Enhanced Summary**:
    *   Use the `GET /chroma/summary` endpoint with the same query.
    *   Observe the natural language summary generated by your local Ollama model.

## 7. Troubleshooting

*   **`Connection Refused` Error**: If your FastAPI app cannot connect to Ollama, ensure:
    *   Ollama server is running.
    *   No firewall is blocking `http://localhost:11434`.
*   **`Model Not Found` Error**: Ensure you have pulled the exact models specified in `chroma_service.py` using `ollama pull <model_name>`.
*   **Poor Quality Summaries**:
    *   Experiment with different small LLMs from `ollama.com/library`.
    *   Adjust the `prompt` in `generate_natural_language_response` to better guide the chosen LLM.
    *   Adjust `temperature` and `num_predict` options in the Ollama `chat` call.
*   **High Resource Usage**: Monitor your system's RAM and CPU. If performance is an issue, consider smaller embedding or generation models, or ensure you have sufficient resources.
